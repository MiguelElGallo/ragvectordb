{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install python-dotenv langchain langchain-community langchainhub openai tiktoken azure-ai-documentintelligence azure-identity azure-search-documents==11.4.0b8\n",
    "! pip install langchain --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ScoringProfile,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    TextWeights)\n",
    "\n",
    "# Define fileds for the index of the corpus\n",
    "\n",
    "fields = [\n",
    "SimpleField(\n",
    "    name=\"id\",\n",
    "    type=SearchFieldDataType.String,\n",
    "    key=True,\n",
    "    filterable=True,\n",
    "),\n",
    "SearchableField(\n",
    "    name=\"content\",\n",
    "    type=SearchFieldDataType.String,\n",
    "    searchable=True,\n",
    "),\n",
    "SearchField(\n",
    "    name=\"content_vector\",\n",
    "    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "    searchable=True,\n",
    "    vector_search_dimensions=1536,\n",
    "    vector_search_configuration=\"default\",\n",
    "),\n",
    "SearchableField(\n",
    "    name=\"metadata\",\n",
    "    type=SearchFieldDataType.String,\n",
    "    searchable=True,\n",
    "),\n",
    "# Additional field to store which engine got an answer\n",
    "SearchableField(\n",
    "    name=\"engine\",\n",
    "    type=SearchFieldDataType.String,\n",
    "    searchable=True,\n",
    "),\n",
    "# Additional field for filtering on document source\n",
    "SimpleField(\n",
    "    name=\"source\",\n",
    "    type=SearchFieldDataType.String,\n",
    "    filterable=True,\n",
    "),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "# Called multiple times in differenct cells, you can run them independently\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "def set_env():\n",
    "    load_dotenv()  # take environment variables from .env.\n",
    "    os.environ[\"openai.api_type\"] = os.getenv(\"openai.api_type\")\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    global doc_intelligence_endpoint\n",
    "    doc_intelligence_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "\n",
    "    global doc_intelligence_key\n",
    "    doc_intelligence_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "\n",
    "    global api_version\n",
    "    api_version = os.getenv(\"openai.api_version\")\n",
    "\n",
    "    global ada_deployed_model \n",
    "    ada_deployed_model = os.getenv(\"ada\")\n",
    "\n",
    "    global gpt4_deployed_model\n",
    "    gpt4_deployed_model = os.getenv(\"gpt4\")\n",
    "    \n",
    "    global vector_store_address\n",
    "    vector_store_address = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "\n",
    "    global vector_store_password\n",
    "    vector_store_password = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "\n",
    "set_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the function to chunk and embed a document\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "def process_file(file_path, index_name):\n",
    "    # Set environment variables\n",
    "    set_env()\n",
    "\n",
    "    # Initiate Azure AI Document Intelligence to load the document. You can either specify file_path or url_path to load the document.\n",
    "    loader = AzureAIDocumentIntelligenceLoader(\n",
    "        file_path=file_path,\n",
    "        api_key=doc_intelligence_key,\n",
    "        api_endpoint=doc_intelligence_endpoint,\n",
    "        api_model=\"prebuilt-layout\",\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split the document into chunks base on markdown headers.\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "    text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "    docs_string = docs[0].page_content\n",
    "    splits = text_splitter.split_text(docs_string)\n",
    "\n",
    "    print(\"Length of splits: \" + str(len(splits)))\n",
    "\n",
    "    # Embed the splitted documents and insert into Azure Search vector store\n",
    "    # openai.api_base = os.getenv(\"openai.api_base\")\n",
    "    import openai\n",
    "\n",
    "    openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "    aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=ada_deployed_model,\n",
    "        openai_api_version=api_version,  # e.g., \"2023-07-01-preview\"\n",
    "    )\n",
    "\n",
    "    vector_store: AzureSearch = AzureSearch(\n",
    "        azure_search_endpoint=vector_store_address,\n",
    "        azure_search_key=vector_store_password,\n",
    "        index_name=index_name,\n",
    "        embedding_function=aoai_embeddings.embed_query,\n",
    "    )\n",
    "\n",
    "    vector_store.add_documents(documents=splits)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of splits: 1\n",
      "Length of splits: 2\n"
     ]
    }
   ],
   "source": [
    "# Let's create two indexes \n",
    "# You can comment out this two lines, after the first run.\n",
    "# After the first indexes will exist in your Azure Search service. No need to create them again.\n",
    "# Here for practicality, we create two indexes, but in the final implementation, this could be totally different vector databases/stores\n",
    "process_file(\"../sample_docs/pdf-sample.pdf\", \"pdf-sample\")\n",
    "process_file(\"../sample_docs/Mac.pdf\", \"mac\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the function to store prompts and which index to search\n",
    "# This define our corpus of knowledge , of which embedding store (vector store or db) to search\n",
    "\n",
    "def feed_corpus(question, engine):\n",
    "    # Set environment variables\n",
    "    set_env()\n",
    "\n",
    "    import openai\n",
    "\n",
    "    openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "    aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=ada_deployed_model,\n",
    "        openai_api_version=api_version,  # e.g., \"2023-07-01-preview\"\n",
    "    )\n",
    "\n",
    "    vector_store_address: str = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "    vector_store_password: str = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "    vector_store: AzureSearch = AzureSearch(\n",
    "        azure_search_endpoint=vector_store_address,\n",
    "        azure_search_key=vector_store_password,\n",
    "        index_name=\"corpus\",\n",
    "        embedding_function=aoai_embeddings.embed_query,\n",
    "        fields=fields\n",
    "    )\n",
    "\n",
    "    \n",
    "    vector_store.add_texts(\n",
    "        [question],\n",
    "        [\n",
    "            { \"content\": question, \"engine\": engine}\n",
    "        ]\n",
    "        \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_corpus(question,  top=3):\n",
    "    # Set environment variables\n",
    "    set_env()\n",
    "    import openai\n",
    "\n",
    "    openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "    aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=ada_deployed_model,\n",
    "        openai_api_version=api_version,  # e.g., \"2023-07-01-preview\"\n",
    "    )\n",
    "\n",
    "    vector_store: AzureSearch = AzureSearch(\n",
    "        azure_search_endpoint=vector_store_address,\n",
    "        azure_search_key=vector_store_password,\n",
    "        index_name=\"corpus\",\n",
    "        embedding_function=aoai_embeddings.embed_query,\n",
    "        fields=fields\n",
    "    )\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "    retrieved_docs = retriever.get_relevant_documents(\n",
    "        \"<your question>\"\n",
    "    )\n",
    "\n",
    "    if len(retrieved_docs) == 0: # The corpus does not know which DB has the answer\n",
    "        return 1,\"none\"\n",
    "    \n",
    "    # there could be multiple answers, we just take the first one in this version\n",
    "\n",
    "    print(retrieved_docs[0].page_content)\n",
    "    print(retrieved_docs[0].metadata)\n",
    "    \n",
    "    engine = retrieved_docs[0].metadata[\"engine\"]\n",
    "    return 0, engine\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who can read a PDF file?\n",
      "{'id': 'MmYwYThkOWUtNTE0OC00MDQyLTlkYWYtZDM1OTg3MDcxN2Nk', 'content': 'Who can read a PDF file?', 'engine': 'pdf-sample'}\n",
      "(0, 'pdf-sample')\n"
     ]
    }
   ],
   "source": [
    "# Cell for testing the functions\n",
    "# feed_corpus(\"Who can read a PDF file?\", \"pdf-sample\")\n",
    "print(ask_corpus(\"Who can read a PDF file?\",3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for setting up the RAG \n",
    "def ask_llm_rag(index_name,question):\n",
    "    found = -1 \n",
    "    aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=ada_deployed_model,\n",
    "        openai_api_version=api_version,  # e.g., \"2023-07-01-preview\"\n",
    "    )\n",
    "    # Set environment variables\n",
    "    vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=aoai_embeddings.embed_query,\n",
    "    )\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "    retrieved_docs = retriever.get_relevant_documents(\n",
    "        \"<your question>\"\n",
    "    )\n",
    "\n",
    "    if len(retrieved_docs) == 0: # This vector store does not has anwers\n",
    "       found = 1\n",
    "    else:\n",
    "       found = 0\n",
    "\n",
    "    # Use a prompt for RAG that is checked into the LangChain prompt hub (https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=989ad331-949f-4bac-9694-660074a208a7)\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    llm = AzureChatOpenAI(\n",
    "        openai_api_version=api_version,  # e.g., \"2023-07-01-preview\"\n",
    "        azure_deployment=gpt4_deployed_model,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "    rag_chain_pdf = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )   \n",
    "    answer = rag_chain_pdf.invoke(question)\n",
    "    return(found,answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anyone, anywhere can read a PDF file. All that is needed is the free Adobe Acrobat Reader. This software is easy to download and can be freely distributed by anyone.\n"
     ]
    }
   ],
   "source": [
    "# Cell for testing the RAG\n",
    "found, ans = ask_llm_rag(\"pdf-sample\", \"Who can read a PDF file?\")\n",
    "print(ans)\n",
    "print(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple index (in the future) multile vector stores\n",
    "# The corpus is the vector store that contains which vector store has the answer\n",
    "\n",
    "# We will do two iterartions\n",
    "# First interation our corupus will be empty, we will need to query al vector stores\n",
    "# Second iteration we will have a corpus and we will know which vector store has the answer\n",
    "\n",
    "# Fist iteration\n",
    "vetor_stores = [\"pdf-sample\", \"mac\"]\n",
    "question = \"Who can read a PDF file?\"\n",
    "for store in vetor_stores:\n",
    "    print(ask_llm_rag(store, question))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
