{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install python-dotenv langchain langchain_openai langchain-community langchainhub openai tiktoken azure-ai-documentintelligence azure-identity azure-search-documents==11.4.0b8\n",
    "! pip install langchain --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ScoringProfile,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    TextWeights,\n",
    ")\n",
    "\n",
    "# Define fileds for the index of the corpus\n",
    "\n",
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        filterable=True,\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"content\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"content_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=1536,\n",
    "        vector_search_configuration=\"default\",\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"metadata\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    # Additional field to store which engine got an answer\n",
    "    SearchableField(\n",
    "        name=\"engine\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    # Additional field for filtering on document source\n",
    "    SimpleField(\n",
    "        name=\"source\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        filterable=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "vector_stores = [\"pdf-sample\", \"mac\"]  # List of vector stores to query for answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "# Called multiple times in differenct cells, you can run them independently\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "def set_env():\n",
    "    load_dotenv()  # take environment variables from .env.\n",
    "    os.environ[\"openai.api_type\"] = os.getenv(\"openai.api_type\")\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    global doc_intelligence_endpoint\n",
    "    doc_intelligence_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "\n",
    "    global doc_intelligence_key\n",
    "    doc_intelligence_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "\n",
    "    global api_version\n",
    "    api_version = os.getenv(\"openai.api_version\")\n",
    "\n",
    "    global ada_deployed_model\n",
    "    ada_deployed_model = os.getenv(\"ada\")\n",
    "\n",
    "    global gpt4_deployed_model\n",
    "    gpt4_deployed_model = os.getenv(\"gpt4\")\n",
    "\n",
    "    global vector_store_address\n",
    "    vector_store_address = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "\n",
    "    global vector_store_password\n",
    "    vector_store_password = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "\n",
    "\n",
    "set_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the function to chunk and embed a document\n",
    "\n",
    "\n",
    "def process_file(file_path, index_name):\n",
    "    # Set environment variables\n",
    "    set_env()\n",
    "\n",
    "    # Initiate Azure AI Document Intelligence to load the document. You can either specify file_path or url_path to load the document.\n",
    "    loader = AzureAIDocumentIntelligenceLoader(\n",
    "        file_path=file_path,\n",
    "        api_key=doc_intelligence_key,\n",
    "        api_endpoint=doc_intelligence_endpoint,\n",
    "        api_model=\"prebuilt-layout\",\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split the document into chunks base on markdown headers.\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "    text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "    docs_string = docs[0].page_content\n",
    "    splits = text_splitter.split_text(docs_string)\n",
    "\n",
    "    print(\"Length of splits: \" + str(len(splits)))\n",
    "\n",
    "    # Embed the splitted documents and insert into Azure Search vector store\n",
    "    # openai.api_base = os.getenv(\"openai.api_base\")\n",
    "    import openai\n",
    "\n",
    "    openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "    aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=ada_deployed_model,\n",
    "        openai_api_version=api_version,  # e.g., \"2023-07-01-preview\"\n",
    "    )\n",
    "\n",
    "    vector_store: AzureSearch = AzureSearch(\n",
    "        azure_search_endpoint=vector_store_address,\n",
    "        azure_search_key=vector_store_password,\n",
    "        index_name=index_name,\n",
    "        embedding_function=aoai_embeddings.embed_query,\n",
    "    )\n",
    "\n",
    "    vector_store.add_documents(documents=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create two indexes\n",
    "# You can comment out this two lines, after the first run.\n",
    "# After the first indexes will exist in your Azure Search service. No need to create them again.\n",
    "# Here for practicality, we create two indexes, but in the final implementation, this could be totally different vector databases/stores\n",
    "process_file(\"../sample_docs/pdf-sample.pdf\", \"pdf-sample\")\n",
    "process_file(\"../sample_docs/Mac.pdf\", \"mac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the function to store prompts and which index to search\n",
    "# This define our corpus of knowledge , of which embedding store (vector store or db) to search\n",
    "\n",
    "\n",
    "def feed_corpus(question, engine):\n",
    "    # Set environment variables\n",
    "    set_env()\n",
    "\n",
    "    import openai\n",
    "\n",
    "    openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "    aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=ada_deployed_model,\n",
    "        openai_api_version=api_version,  # e.g., \"2023-07-01-preview\"\n",
    "    )\n",
    "\n",
    "    vector_store_address: str = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "    vector_store_password: str = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "\n",
    "    vector_store: AzureSearch = AzureSearch(\n",
    "        azure_search_endpoint=vector_store_address,\n",
    "        azure_search_key=vector_store_password,\n",
    "        index_name=\"corpus\",\n",
    "        embedding_function=aoai_embeddings.embed_query,\n",
    "        fields=fields,\n",
    "    )\n",
    "\n",
    "    vector_store.add_texts([question], [{\"content\": question, \"engine\": engine}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_corpus(question, top=3):\n",
    "    # Set environment variables\n",
    "    set_env()\n",
    "    import openai\n",
    "\n",
    "    openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "    aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=ada_deployed_model,\n",
    "        openai_api_version=api_version,  # e.g., \"2023-07-01-preview\"\n",
    "    )\n",
    "\n",
    "    vector_store: AzureSearch = AzureSearch(\n",
    "        azure_search_endpoint=vector_store_address,\n",
    "        azure_search_key=vector_store_password,\n",
    "        index_name=\"corpus\",\n",
    "        embedding_function=aoai_embeddings.embed_query,\n",
    "        fields=fields,\n",
    "    )\n",
    "\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    )\n",
    "\n",
    "    retrieved_docs = retriever.get_relevant_documents(\"<your question>\")\n",
    "\n",
    "    if len(retrieved_docs) == 0:  # The corpus does not know which DB has the answer\n",
    "        return 1, \"none\"\n",
    "\n",
    "    # there could be multiple answers, we just take the first one in this versio\n",
    "    engine = retrieved_docs[0].metadata[\n",
    "        \"engine\"\n",
    "    ]  # Our custom fields, comntains the name of the Vector Store\n",
    "\n",
    "    return 0, engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for testing the functions\n",
    "# feed_corpus(\"Who can read a PDF file?\", \"pdf-sample\")\n",
    "# print(ask_corpus(\"Who can read a PDF file?\", 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for setting up the RAG\n",
    "def ask_llm_rag(index_name, question):\n",
    "    # Set environment variables\n",
    "    set_env()\n",
    "\n",
    "    found = -1\n",
    "    aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_deployment=ada_deployed_model,\n",
    "        openai_api_version=api_version,  # e.g., \"2023-07-01-preview\"\n",
    "    )\n",
    "    # Set environment variables\n",
    "    vector_store: AzureSearch = AzureSearch(\n",
    "        azure_search_endpoint=vector_store_address,\n",
    "        azure_search_key=vector_store_password,\n",
    "        index_name=index_name,\n",
    "        embedding_function=aoai_embeddings.embed_query,\n",
    "    )\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}\n",
    "    )\n",
    "\n",
    "    retrieved_docs = retriever.get_relevant_documents(\"<your question>\")\n",
    "\n",
    "    if len(retrieved_docs) == 0:  # This vector store does not has anwers\n",
    "        found = 1\n",
    "    else:\n",
    "        found = 0  # This vector store has anwers (success, exit code 0, unix style :) )\n",
    "\n",
    "    # Use a prompt for RAG that is checked into the LangChain prompt hub (https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=989ad331-949f-4bac-9694-660074a208a7)\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    llm = AzureChatOpenAI(\n",
    "        openai_api_version=api_version,  # e.g., \"2023-07-01-preview\"\n",
    "        azure_deployment=gpt4_deployed_model,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain_pdf = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = rag_chain_pdf.invoke(question)\n",
    "    if answer.find('The provided context does not contain information') != -1 :\n",
    "        found = 1  # This vector store does not has anwers\n",
    "    if answer.find('The context provided does not contain information') != -1 :\n",
    "        found = 1 # This vector store does not has anwers\n",
    "    # Improve this part! \n",
    "    return (found, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for testing the RAG\n",
    "# (found, ans) = ask_llm_rag(\"pdf-sample\", \"Who can read a PDF file?\")\n",
    "# print(ans)\n",
    "# print(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query multiple vector stores, first the corpus, then all vector stores. Also update the corpus if the answer is found in a vector store.\n",
    "\n",
    "\n",
    "def query_with_corpus(question):\n",
    "    # Set environment variables\n",
    "    set_env()\n",
    "    print(\"  -- Asking the reference corpus first -- \")\n",
    "    (found, vector_db) = ask_corpus(question)\n",
    "    if found == 0:  # Found in that vector store (success, exit code 0, unix style :) )\n",
    "        skip_scan_alls = True\n",
    "        print(\n",
    "            \"    -- The corpus knows which vector store has information about:  \"\n",
    "            + question\n",
    "            + \" -- \"\n",
    "        )\n",
    "        print(\"    -- Querying specific vector store: \" + vector_db)\n",
    "        (found, answer) = ask_llm_rag(vector_db, question)\n",
    "        if (\n",
    "            found == 0\n",
    "        ):  # Found in that vector store (success, exit code 0, unix style :) )\n",
    "            print(\"The answer is: \" + answer)\n",
    "\n",
    "        else:\n",
    "            print(\n",
    "                \"Vector store \"\n",
    "                + vector_db\n",
    "                + \" does not have the answer for the question: \"\n",
    "                + question\n",
    "            )\n",
    "    else:\n",
    "        print(\n",
    "            \"    -- The corpus does not know which vector store has information about:  \"\n",
    "            + question\n",
    "            + \" -- \"\n",
    "        )\n",
    "        print(\"    -- Querying all vector stores -- \")\n",
    "        skip_scan_alls = False\n",
    "\n",
    "    if skip_scan_alls is False:\n",
    "        for store in vector_stores:\n",
    "            print(\n",
    "                \"  -- All vector stores will be queried, now Querying vector store: \"\n",
    "                + store\n",
    "            )\n",
    "            (found, answer) = ask_llm_rag(store, question)\n",
    "            if (\n",
    "                found == 0\n",
    "            ):  # Found in that vector store (success, exit code 0, unix style :) )\n",
    "                feed_corpus(question, store)\n",
    "                print(\n",
    "                    \"    -- The corpus has been updated: \"\n",
    "                    + store\n",
    "                    + \" has knowledge about the answer for the question: \"\n",
    "                    + question\n",
    "                    + \" -- \"\n",
    "                )\n",
    "                print(\"The answer is: \" + answer)\n",
    "            else:\n",
    "                print(\n",
    "                    \"    --  Vector store \"\n",
    "                    + store\n",
    "                    + \" does not have the answer for the question: \"\n",
    "                    + question\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple index (in the future) multile vector stores\n",
    "# The corpus is the vector store that contains which vector store has the answer\n",
    "\n",
    "# We will do two iterartions\n",
    "# First interation our corupus will be empty, we will need to query al vector stores\n",
    "# Second iteration we will have a corpus and we will know which vector store has the answer\n",
    "\n",
    "# First iteration\n",
    "print(\"-- First iteration -- \")\n",
    "query_with_corpus(\"Describe characteristics of an iMac G3?\")\n",
    "\n",
    "print(\"-- Second iteration -- \")\n",
    "# Second iteration\n",
    "# We expected to corpus to know which vector store has the answer, avoiding querying all vector stores\n",
    "query_with_corpus(\"Describe characteristics of an iMac G3?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
